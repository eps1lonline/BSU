import numpy as np

def nonlin(x, deriv=False):
    if deriv:
        return x * (1 - x)  # Производная сигмоиды
    return 1 / (1 + np.exp(-x))  # Сигмоида

# Входные данные
X = np.array([[1,1,1,1,1,1,0, 1,0,0,0,0,0,0, 1,0,0,0,0,0,0, 1,1,1,1,1,0,0, 1,0,0,0,0,0,0, 1,0,0,0,0,0,0, 1,0,0,0,0,0,0, 1,1,1,1,1,1,0], # E
              [1,0,0,0,0,1,0, 1,0,0,0,0,1,0, 1,0,0,0,0,1,0, 1,1,1,1,1,1,0, 1,0,0,0,0,1,0, 1,0,0,0,0,1,0, 1,0,0,0,0,1,0, 1,0,0,0,0,1,0], # H
              [0,0,1,1,0,0,0, 0,1,0,0,1,0,0, 0,1,0,0,1,0,0, 0,1,0,0,1,0,0, 0,1,0,0,1,0,0, 0,1,0,0,1,0,0, 1,1,1,1,1,1,0, 1,0,0,0,0,1,0]]) # Д

# Выходные данные
y = np.array([[1, 0, 0], # Е
              [0, 1, 0], # Н
              [0, 0, 1]]) # Д

np.random.seed(1)

# Инициализация весов
input_layer_size = X.shape[1]  # Количество входных нейронов
hidden_layer_size_1 = 5         # Количество нейронов в первом скрытом слое
hidden_layer_size_2 = 4         # Количество нейронов во втором скрытом слое
output_layer_size = y.shape[1]  # Количество выходных нейронов

syn0 = 2 * np.random.random((input_layer_size, hidden_layer_size_1)) - 1  # Входной слой к 1-му скрытому слою
syn1 = 2 * np.random.random((hidden_layer_size_1, hidden_layer_size_2)) - 1  # 1-й скрытый слой ко 2-му скрытому слою
syn2 = 2 * np.random.random((hidden_layer_size_2, output_layer_size)) - 1  # 2-й скрытый слой к выходному слою

# Обучение сети
for j in range(60000):
    # Прямое распространение
    l0 = X
    l1 = nonlin(np.dot(l0, syn0))  # Первый скрытый слой
    l2 = nonlin(np.dot(l1, syn1))   # Второй скрытый слой
    l3 = nonlin(np.dot(l2, syn2))   # Выходной слой

    # Вычисление ошибки
    l3_error = y - l3
    
    if (j % 10000) == 0:
        print("Error:" + str(np.mean(np.abs(l3_error))))
        
    # Обратное распространение
    l3_delta = l3_error * nonlin(l3, deriv=True)  # Ошибка на выходе
    l2_error = l3_delta.dot(syn2.T)  # Ошибка для второго скрытого слоя
    l2_delta = l2_error * nonlin(l2, deriv=True)  # Ошибка с учетом производной
    l1_error = l2_delta.dot(syn1.T)  # Ошибка для первого скрытого слоя
    l1_delta = l1_error * nonlin(l1, deriv=True)  # Ошибка с учетом производной

    # Обновление весов
    syn2 += l2.T.dot(l3_delta)  # Обновление весов между вторым скрытым и выходным слоями
    syn1 += l1.T.dot(l2_delta)  # Обновление весов между первым и вторым скрытыми слоями
    syn0 += l0.T.dot(l1_delta)  # Обновление весов между входным и первым скрытыми слоями

# Вывод результатов после тренировки
print("Выходные данные после тренировки:")
print(np.round(l3, 6))


#Проверка
def predict_letter(l0_1):
    l1_1 = nonlin(np.dot(l0_1, syn0))  # Первый скрытый слой
    l2_1 = nonlin(np.dot(l1_1, syn1))   # Второй скрытый слой
    l3_1 = nonlin(np.dot(l2_1, syn2))
    return np.round(l3_1, 4)

X1 = np.array([[1,1,1,1,1,1,0, 1,0,0,0,0,0,0, 1,0,0,0,0,0,0, 1,1,1,1,1,0,0, 1,0,0,0,0,0,0, 1,0,0,0,0,0,0, 1,0,0,0,0,0,0, 1,1,1,1,1,1,0], # E
              [1,0,0,0,0,1,0, 1,0,0,0,0,1,0, 1,0,0,0,0,1,0, 1,1,1,1,1,1,0, 1,0,0,0,0,1,0, 1,0,0,0,0,1,0, 1,0,0,0,0,1,0, 1,0,0,0,0,1,0], # H
              [0,0,1,1,0,0,0, 0,1,0,0,1,0,0, 0,1,0,0,1,0,0, 0,1,0,0,1,0,0, 0,1,0,0,1,0,0, 0,1,0,0,1,0,0, 1,1,1,1,1,1,0, 1,0,0,0,0,1,0], # Д
              [1,0,0,0,0,1,0, 1,0,0,0,1,0,0, 1,0,0,1,0,0,0, 1,0,1,0,0,0,0, 1,1,1,0,0,0,0, 1,0,0,1,0,0,0, 1,0,0,0,1,0,0, 1,0,0,0,0,1,0]]) # K

print(predict_letter(X1))   